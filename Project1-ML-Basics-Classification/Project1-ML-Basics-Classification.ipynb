{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIN313 ASSINGMENT 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Theory Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecommunication Customer Classification System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __STEP 1__\n",
    "__1. Import and visualize the data in any aspects that you think it is beneficial for the reader’s better understanding of the data.__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset and show it\n",
    "df = pd.read_csv('telecommunicaton_classification.csv')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the target column (service)\n",
    "sns.countplot(x='service', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and normalize the DataFrame excluding the service column\n",
    "df_except_service = df.iloc[:,:-1]\n",
    "\n",
    "dummies = pd.get_dummies(df_except_service, drop_first=True, dtype=int)\n",
    "dummies_normalized = (dummies - dummies.min()) / (dummies.max() - dummies.min())\n",
    "\n",
    "df = pd.concat([dummies, df.iloc[:, -1]], axis=1)\n",
    "\n",
    "n_df = pd.concat([dummies_normalized, df.iloc[:, -1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show and Check transformed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __STEP 2__ \n",
    "__2. Split data into train and test set randomly (you can use 80% of the data for training and 20% of it for the test purposes).__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features (X) and target (y) based on non_normalized df (df)\n",
    "X = df.drop('service', axis=1)  \n",
    "y = df['service']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features (X) and target (y) based on normalized df (ndf/NDF)\n",
    "X_NDF = n_df.drop('service', axis=1)  \n",
    "y_ndf = n_df['service']\n",
    "\n",
    "# Split data\n",
    "X_train_ndf, X_test_ndf, y_train_ndf, y_test_ndf = train_test_split(X_NDF, y_ndf, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __STEP 3 , STEP 4 and STEP 5__\n",
    "\n",
    "__3. For the test set that you separated at the previous step try to determine classes for the customers.__ <br />\n",
    "__4. Finally compute performance of your model to measure the success of your KNN Classification method for each setting you have used: You will report Accuracy, Precision, and Recall measures.__ <br />\n",
    "__5. The most important part of this project is doing as much experiment as you can to show strengths and weaknesses of the kNN algorithm.__ <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance functions\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary functions to knn\n",
    "\n",
    "def nearest_neighbors(distance_class, k):\n",
    "    neighbors = [] # where we keep the k nearest neighbors\n",
    "\n",
    "    distance_class.sort(key=lambda x: x[0])  # We sort by distance.\n",
    "    for dis , cls in distance_class[:k]:\n",
    "        neighbors.append(cls)\n",
    "    return neighbors\n",
    "\n",
    "def most_common_neighbor(neighbors):\n",
    "    neighbor = Counter(neighbors).most_common(1)[0][0] # The most common neighbor among the k nearest neighbors\n",
    "    return neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Algorithm function\n",
    "\n",
    "def k_nearest_neighbors(X_train, y_train, X_test, k, distance_func):\n",
    "    predictions = []  # We'll keep our guesses here\n",
    "    \n",
    "    for test_point in X_test.to_numpy():  \n",
    "        distances = []  # A list to store distances\n",
    "        \n",
    "        # We calculate the distance with the data points in the training set.\n",
    "        for i, train_point in enumerate(X_train.to_numpy()):\n",
    "            distance = distance_func(test_point, train_point)\n",
    "            distances.append((distance, y_train.iloc[i]))  # We keep distance and class.\n",
    "        \n",
    "        # We call the k nearest neighbors.\n",
    "        neighbors = nearest_neighbors(distances , k)\n",
    "        \n",
    "        # We call the most common class and throw it as a guess.\n",
    "        neighbor = most_common_neighbor(neighbors)\n",
    "        \n",
    "        predictions.append(neighbor)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_k_values(X_train, y_train, X_test, y_test, k_values , distance_func):\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        y_pred = k_nearest_neighbors(X_train, y_train, X_test, k , distance_func)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        print(f'k={k}, Accuracy={accuracy:.2f}')\n",
    "    \n",
    "    return accuracy_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the efficiency of the knn algorithm according to normalized and non-normalized data frame (with euclidean distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_k_func(X_train, y_train, X_test, y_test ,distance_func):\n",
    "    # k values ​​to try\n",
    "    k_values = range(1, 100, 2)\n",
    "\n",
    "    # We evaluate the model for each k.\n",
    "    accuracy_scores = evaluate_k_values(X_train, y_train, X_test, y_test, k_values , distance_func)\n",
    "\n",
    "    # We find the best k value.\n",
    "    best_k = k_values[np.argmax(accuracy_scores)]\n",
    "    print(f'Best k value: {best_k}')\n",
    "\n",
    "    # We draw graphs according to the accuracy scores of the k values ​​we tried.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, accuracy_scores, marker='o', linestyle='--', color='b')\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title('Accuracy Score According to Different k Values')\n",
    "    plt.xticks(k_values)  \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return best_k\n",
    "\n",
    "def scores(X_train, y_train, X_test, y_test, best_k , distance_func):\n",
    "    # We make the model predict.\n",
    "    y_pred = k_nearest_neighbors(X_train, y_train, X_test, best_k , distance_func)\n",
    "\n",
    "    # We are evaluating the results. \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn algorithm for non-normalized df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = best_k_func(X_train, y_train, X_test, y_test , euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(X_train, y_train, X_test, y_test, best_k , euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn algorithm for normalized df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = best_k_func(X_train_ndf, y_train_ndf, X_test_ndf, y_test_ndf, euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(X_train_ndf, y_train_ndf, X_test_ndf, y_test_ndf, best_k, euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Results: Normalized vs. Non-Normalized Data\n",
    "\n",
    "#### 1. General Performance Comparison\n",
    "- **Non-Normalized Data**: Accuracy ranges from 0.30 to 0.39. The best k value is 37 with 0.39 accuracy.\n",
    "- **Normalized Data**: Accuracy is generally higher, between 0.30 and 0.43. The best k value is 87 with 0.43 accuracy.\n",
    "- Normalization improves performance and stability, especially at higher k values.\n",
    "\n",
    "#### 2. Optimal k Value\n",
    "- **Non-Normalized**: Best k is 37 (0.39 accuracy). Accuracy improves with higher k values but plateaus beyond a certain point.\n",
    "- **Normalized**: Best k is 87 (0.43 accuracy). Large k values work better, indicating stronger generalization due to normalization.\n",
    "\n",
    "#### 3. Overfitting vs. Generalization\n",
    "- **Non-Normalized**: Small k values lead to overfitting (e.g., k=1), while larger k values generalize better.\n",
    "- **Normalized**: Smaller k values are more stable, and higher k values (e.g., k=87) show the best performance.\n",
    "\n",
    "#### 4. Impact of Normalization\n",
    "- Normalization results in consistently higher accuracy and less sensitivity to k value changes.\n",
    "- Larger k values perform better on normalized data.\n",
    "\n",
    "#### 5. Conclusion\n",
    "- Normalization improves kNN's performance, especially with large k values.\n",
    "- **Recommendation**: Use normalization to enhance accuracy and generalization in kNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the efficiency of the knn algorithm according to distance function difference (with normalized df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = best_k_func(X_train_ndf, y_train_ndf, X_test_ndf, y_test_ndf, manhattan_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(X_train_ndf, y_train_ndf, X_test_ndf, y_test_ndf, best_k, manhattan_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = best_k_func(X_train_ndf, y_train_ndf, X_test_ndf, y_test_ndf, euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(X_train_ndf, y_train_ndf, X_test_ndf, y_test_ndf, best_k, euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Results: Manhattan vs. Euclidean Distance\n",
    "\n",
    "#### 1. General Performance Comparison\n",
    "\n",
    "- **Manhattan**: Accuracy ranges from 0.29 to 0.41, with the best k at 99 achieving 0.41 accuracy.\n",
    "- **Euclidean**: Accuracy ranges from 0.30 to 0.43, with the best k at 87 achieving 0.43 accuracy.\n",
    "- Euclidean shows higher overall performance and faster improvement in accuracy with increasing k values.\n",
    "\n",
    "#### 2. Optimal k Value\n",
    "\n",
    "- **Manhattan**: Best k = 99 (accuracy = 0.41). Larger k values provide better generalization but limited improvement.\n",
    "- **Euclidean**: Best k = 87 (accuracy = 0.43). Euclidean improves faster and gives higher accuracy at medium to large k values.\n",
    "\n",
    "#### 3. Overfitting vs. Generalization\n",
    "\n",
    "- **Manhattan**: Lower k values (e.g., k=1) show overfitting, while larger k values provide more generalization.\n",
    "- **Euclidean**: Small k values improve accuracy more quickly, with better generalization at larger k values.\n",
    "\n",
    "#### 4. Manhattan vs. Euclidean\n",
    "\n",
    "- **Euclidean Distance** performs better overall, especially at larger k values (best accuracy = 0.43).\n",
    "- **Manhattan Distance** stabilizes with higher k but doesn't improve beyond 0.41 accuracy.\n",
    "  \n",
    "#### 5. Conclusion\n",
    "\n",
    "- Euclidean is the better choice for kNN, offering faster and higher accuracy improvements at larger k values. It generalizes better compared to Manhattan.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
